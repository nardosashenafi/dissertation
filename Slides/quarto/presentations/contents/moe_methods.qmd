# Mixture of Expert Controller

## Mixture of Expert Controller

<br/>

* **Objective**: learn contact-aware mixture of expert controller and a gating network 
* We pose the search over the parameters $\psi, \theta$ as the following optimization problem

<br/> 

:::{.callout-tip icon="false"}
## Optimization Problem
\begin{align*}
    \begin{aligned}
        \underset{\psi, \theta}{\textrm{minimize}} 
        & & & \int_0^T \ell (x(t),u) \dd t , \\%
        \textrm{subject to}
        & & & M(q) \dd \dot{q} + h(x; \psi, \theta)\dd t - \dd R  = 0,\\%
        & & & u = \{F_i(x; \theta_i) \; | \; i  \sim \text{Categorical} (\mathbf{P}(x| \psi)) \}.
    \end{aligned}
\end{align*}
:::

## Training 

<br/>

::: {.incremental}
* Training procedure:
  + Start from initial parameters $(\psi, \theta)$
  + Sample initial state $x_0$
  + Generate trajectories $\phi(x_0, u, T)$ using current parameters
  \begin{gather*}
    i  \sim \text{Categorical} (\mathbf{P}(x| \psi)) \\
    u(x; \psi, \theta) = F_i(x; \theta_i) \\
    M(q) \dd \dot{q} + h(x; \psi, \theta) \dd t - \dd R  = 0 \text{ (Moreau's time stepping)}
  \end{gather*}
  + Assign a running cost $\ell$ to the trajectories based on performance
  + Update parameters $(\psi, \theta)$ to minimize running cost
:::

## Performance Objective

<br/>

\begin{align*}
    \begin{aligned}
        \underset{\psi, \theta}{\textrm{minimize}} 
        & & & \int_0^T \ell (x(t),u) \dd t , \\%
        \textrm{subject to}
        & & & M(q) \dd \dot{q} + h(x; \psi, \theta)\dd t - \dd R  = 0,\\%
        & & & u = \{F_i(x; \theta_i) \; | \; i  \sim \text{Categorical} (\mathbf{P}(x| \psi)) \}.
    \end{aligned}
\end{align*}

:::{.fragment fragment-index=0}
1. Accumulated loss: total quadratic loss from desired state $x^*$
\begin{gather*}
\ell(x, u) = \frac{1}{2}(x - x^*)^\top \mathcal{Q} (x - x^*) + \frac{1}{2} u^\top \mathcal{R} u \\
\mathcal{Q} \succ 0, \mathcal{R} \succeq 0
\end{gather*}
:::

:::{.fragment fragment-index=1}
The correspond likelihood function is
:::

:::{.r-stack}

:::{.fragment .fade-in-then-out fragment-index=1}
$$
\mathbb{L}(\mathbb{D} | \theta, \psi) = \sum_{j=1}^{N} \sum_{i=1}^{N_F} - \| F_i(x_j; \theta_i) - y_j \|^2 P_i(x_j | \psi)
$$
:::

:::{.fragment .fade-in fragment-index=2}
$$
\mathbb{L}(\phi) = \sum_{t=0}^{T} \sum_{i=1}^{N_F} - \ell (x(t+\Delta t), F_i ) P_i (x(t) | \psi ) 
$$

:::
:::

## Performance Objective

::::{.columns}

:::{.column width="60%"}

2. Minimum Loss Trajectory (MTL):

* Accumulated loss may not reflect desired behavior. E.g. Simple pendulum


:::{.fragment fragment-index=0}
* Simple pendulum needs to pump slowly, which would accumulate large cost
:::

:::

:::{.column width="40%"}

:::{.fragment fragment-index=0}
![](contents/assets/mtl.svg){fig-align="right"}
:::

:::
::::


:::{.fragment fragment-index=1}
* MTL encourages trajectories to *eventually* lead to a minimum cost

\begin{align*}
    \begin{gathered}
        t_{min} = \underset{t}{\textrm{inf}} \; \{ \ell(x(t), u): x(t) \in \phi(x_0, u, T) \}  \\
        \mathbb{L}(\phi) = - \frac{\ell(x(t_{min}), u)}{C} \sum_{t=0}^{t_{min}}P_i(x(t) | \psi) 
    \end{gathered} 
\end{align*}
:::

## State Sampling

* **Objective**: efficiently sample the state space and learn MOE controller for all initial states

## State Sampling
<!-- ###################################################################### -->

:::: {.r-stack}

![](contents/assets/neuralpbc/033.svg){.fragment}

<!-- ![](contents/assets/neuralpbc/000.svg)

![](contents/assets/neuralpbc/001.svg){.fragment}

![](contents/assets/neuralpbc/002.svg){.fragment}

![](contents/assets/neuralpbc/003.svg){.fragment}

![](contents/assets/neuralpbc/004.svg){.fragment}

![](contents/assets/neuralpbc/005.svg){.fragment}

![](contents/assets/neuralpbc/006.svg){.fragment}

![](contents/assets/neuralpbc/007.svg){.fragment}

![](contents/assets/neuralpbc/008.svg){.fragment}

![](contents/assets/neuralpbc/009.svg){.fragment}

![](contents/assets/neuralpbc/010.svg){.fragment}

![](contents/assets/neuralpbc/011.svg){.fragment}

![](contents/assets/neuralpbc/012.svg){.fragment}

![](contents/assets/neuralpbc/013.svg){.fragment}

![](contents/assets/neuralpbc/014.svg){.fragment}

![](contents/assets/neuralpbc/015.svg){.fragment}

![](contents/assets/neuralpbc/016.svg){.fragment}

![](contents/assets/neuralpbc/021.svg){.fragment}

![](contents/assets/neuralpbc/022.svg){.fragment}

![](contents/assets/neuralpbc/023.svg){.fragment}

![](contents/assets/neuralpbc/024.svg){.fragment}

![](contents/assets/neuralpbc/025.svg){.fragment}

![](contents/assets/neuralpbc/026.svg){.fragment}

![](contents/assets/neuralpbc/027.svg){.fragment}

![](contents/assets/neuralpbc/032.svg){.fragment}

![](contents/assets/neuralpbc/033.svg){.fragment} -->

::::