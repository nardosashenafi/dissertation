## Bayesian Learning {auto-animate=true}
 
<br/>

* **Objective**: Given finite dataset with inherent noise, we are in search of a regression model

:::: {.columns}

:::{.column width=60%}

:::{.fragment fragment-index=0}
* E.g. Recognizing handwritten digits from images
    + noise due to differences in individual handwriting
:::

:::{.fragment fragment-index=1}
* *Stochastic models* provide generalized predictions 
and report the uncertainty in each prediction
:::

:::

:::{.column width=40%}

:::{.fragment fragment-index=0}
![](contents/assets/handwritten_images.png){.absolute top=250 left=700 width="350" height="200"}
:::

:::
::::

## Training Stochastic Models {auto-animate=true}

:::: {.columns}

:::{.column width=60%}

:::{r-vstack}

:::{.r-stack}

:::{.fragment .fade-in-then-out fragment-index=0}
:::{.callout-important icon=false}
## Expectation Maximization

$$
\begin{aligned}
\underset{P(\theta)}{\text{maximize}} &&& P(\mathbb{D} | \theta)  = \prod_{j=1}^{N} \mathcal{N}(\| F(x_j; \theta) -  y_j \| \; | \; 0, s), \\
\text{subject to} &&
& \theta \sim P(\theta) \\
&& \mathbb{D} &= \{(x_1, y_1), \dots, (x_N, y_N) \}
\end{aligned}
$$
::: 
<!-- callout -->
::: 
<!-- fragment -->

:::{.fragment .fade-in fragment-index=1}
:::{.callout-important icon=false}
## Expectation Maximization

$$
\begin{aligned}
\underset{P(\theta;z)}{\text{maximize}} &&& P(\mathbb{D} | \theta)  = \prod_{j=1}^{N} \mathcal{N}(\| F(x_j; \theta) -  y_j \| \; | \; 0, s), \\
\text{subject to} &&
& \theta \sim P(\theta;z) \\
&& \mathbb{D} &= \{(x_1, y_1), \dots, (x_N, y_N) \}
\end{aligned}
$$
::: 
<!-- callout -->
::: 
<!-- fragment -->
::: 
<!-- r-stack -->

:::{.fragment .fade-in fragment-index=2}
![](contents/assets/overfitting.svg){.absolute top=150 left=0 width="650" height="650"}
::: 
<!-- fragment -->

::: 
<!-- r-vstack -->
::: 
<!-- column -->

:::{.column width=40%}

:::{.r-vstack}
![](contents/assets/handwritten_images.png)

:::{.fragment .fade-in fragment-index=2}
:::{.incremental}
* Expectation maximization is prone to overfitting
    + Reduces accuracy of predictions
    + Reports near-zero prediction uncertainty (*overconfident*)
* **Solution**: enforce variance on the posterior 
::: 
<!--incremental -->

::: 
<!-- fragment -->

::: 
<!-- r-vstack -->

::: 
<!-- column -->
:::: 

## Bias-Variance Trade-Off {auto-animate=true}


<br/>

* Prior distribution plays the role of a *regularization term*

::::{.columns}

:::{.column width=50%}

:::{.fragment fragment-index=0}
:::{.callout-warning icon="false"}
## Posterior Distribution
$$
P(\theta \mid \mathbb{D}) = \frac{\overbrace{P(\mathbb{D} \mid
\theta)}^{\text{likelihood}}\overbrace{P(\theta)}^{\text{prior}}}
{\underbrace{\int_\theta P(\mathbb{D} \mid \theta^\prime)P(\theta^\prime)
d\theta^\prime}_{\text{evidence}}} 
$$ 
:::
:::

:::

:::{.column width=50%}
![](contents/assets/overfitting.svg){.absolute top=0 left=600 width="600" height="600"}
:::

::::

:::{.fragment fragment-index=1}
:::{.incremental}
* The likelihood and the prior are in a tug of war, which prevents the posterior from overfitting on training dataset
* Prior distribution can be
    + *Informed*: allows us to inject prior knowledge
    + *Uninformed*: starts randomly but every so often gets updated by the posterior
:::
:::

## Estimating Posterior Distribution {auto-animate=true}


::::{.columns}

:::{.column width=50%}

:::

:::{.column width=50%}

:::{.r-stack}
:::{.callout-warning icon="false"}
## Posterior Distribution
$$
P(\theta \mid \mathbb{D}) = \frac{\overbrace{P(\mathbb{D} \mid
\theta)}^{\text{likelihood}}\overbrace{P(\theta)}^{\text{prior}}}
{\underbrace{\int_\theta P(\mathbb{D} \mid \theta^\prime)P(\theta^\prime)
d\theta^\prime}_{\text{evidence}}} 
$$ 
:::
<!-- callout -->
:::
<!-- stack -->
:::
<!-- column -->

::::
## Proposed Method



