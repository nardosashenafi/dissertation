## Bayesian Learning {auto-animate=true}
 
<br/>

* **Objective**: Given finite dataset with inherent noise, we are in search of a regression model

:::: {.columns}

:::{.column width=60%}

:::{.fragment fragment-index=0}
* E.g. Recognizing handwritten digits from images
    + noise due to differences in individual handwriting
:::

:::{.fragment fragment-index=1}
* *Stochastic models* provide generalized predictions 
and report the uncertainty in each prediction
    + The model is given by $F(x;\theta)$ where the parameter $\theta$ is a sample drawn from a distribution

:::

:::

:::{.column width=40%}

:::{.fragment fragment-index=0}
![](contents/assets/handwritten_images.png){.absolute top=250 left=700 width="350" height="200"}
:::

:::
::::

## Training Stochastic Models {auto-animate=true}

:::: {.columns}

:::{.column width=60%}

:::{r-vstack}

:::{.r-stack}

:::{.fragment .fade-in-then-out fragment-index=0}
:::{.callout-important icon=false}
## Expectation Maximization

$$
\begin{aligned}
\underset{P(\theta | \mathbb{D})}{\text{maximize}} && P(\mathbb{D} | \theta)  &= \prod_{j=1}^{N} \mathcal{N}(\| F(x_j; \theta) -  y_j \| \; | \; 0, s), \\
\text{subject to} &&
& \theta \sim P(\theta | \mathbb{D}), \\
&& \mathbb{D} &= \{(x_1, y_1), \dots, (x_N, y_N) \}.
\end{aligned}
$$
::: 
<!-- callout -->
::: 
<!-- fragment -->

:::{.fragment .fade-in-then-out fragment-index=1}
:::{.callout-important icon=false}
## Expectation Maximization

$$
\begin{aligned}
\underset{P(\theta| \mathbb{D})}{\text{maximize}} && P(\mathbb{D} | \theta)  &= \prod_{j=1}^{N} \frac{1}{\sqrt{2 \pi s^2}}\exp(-\frac{1}{2s^2}\| F(x_j; \theta) -  y_j \|^2), \\
\text{subject to} &&
& \theta \sim P(\theta| \mathbb{D}), \\
&& \mathbb{D} &= \{(x_1, y_1), \dots, (x_N, y_N) \}.
\end{aligned}
$$
::: 
<!-- callout -->
::: 
<!-- fragment -->

:::{.fragment .fade-in fragment-index=2}
:::{.callout-important icon=false}
## Expectation Maximization

$$
\begin{aligned}
\underset{z}{\text{maximize}} && P(\mathbb{D} | \theta)  &= \prod_{j=1}^{N} \frac{1}{\sqrt{2 \pi s^2}}\exp(-\frac{1}{2s^2}\| F(x_j; \theta) -  y_j \|^2), \\
\text{subject to} &&
& \theta \sim Q(\theta;z), \\
&& \mathbb{D} &= \{(x_1, y_1), \dots, (x_N, y_N) \}.
\end{aligned}
$$
::: 
<!-- callout -->
::: 
<!-- fragment -->
::: 
<!-- r-stack -->


::: 
<!-- r-vstack -->
::: 
<!-- column -->

:::{.column width=40%}

![](contents/assets/handwritten_images.png){.absolute top=50 left=750 width="350" height="200"}


::: 
<!-- column -->
:::: 

:::: {.columns}

:::{.column width=60%}
:::{.fragment .fade-in fragment-index=3}
![](contents/assets/overfitting.svg){.absolute top=150 left=0 width="650" height="650"}
::: 
<!-- fragment -->
:::

:::{.column width=40%}

:::{.fragment .fade-in fragment-index=3}
:::{.incremental}
* Expectation maximization is prone to overfitting
    + Reduces accuracy of predictions
    + Reports near-zero prediction uncertainty (*overconfident*)
* **Solution**: enforce variance on the posterior 
::: 
<!--incremental -->

::: 
<!-- fragment -->
:::

::::

## Bias-Variance Trade-Off {auto-animate=true}


<br/>

* Prior distribution plays the role of a *regularization term*

:::{.fragment .fade-in fragment-index=0}
:::{.callout-important icon=false}
## Bayesian Inference

$$
\begin{aligned}
\underset{z}{\text{maximize}} &&& P(\mathbb{D} | \theta) P(\theta), \\
\text{subject to} &&
& \theta \sim Q(\theta;z), \\
&& \mathbb{D} &= \{(x_1, y_1), \dots, (x_N, y_N) \}.
\end{aligned}
$$
::: 
<!-- callout -->
:::
<!-- fragment -->

:::{.fragment fragment-index=1}

* Prior distribution can be
    + *Informed*: allows us to inject prior knowledge
    + *Uninformed*: starts randomly but every so often gets updated by the posterior

:::

## Estimating Posterior Distribution {auto-animate=true}

<br/>

:::{.callout-important icon=false}
## Bayesian Inference

$$
\begin{aligned}
\underset{z}{\text{maximize}} &&& P(\mathbb{D} | \theta) P(\theta), \\
\text{subject to} &&
& \theta \sim Q(\theta;z), \\
&& \mathbb{D} &= \{(x_1, y_1), \dots, (x_N, y_N) \}.
\end{aligned}
$$
::: 
<!-- callout -->

* **Variational Inference**: approximates the posterior $P(\theta | \mathbb{D})$ with a pre-selected distribution $Q(\theta; z)$
* **Objective**: collect $N_\theta$ samples from the current posterior $Q(\theta; z)$ and maximize <span style="font-variant:small-caps;">Elbo</span> 
$$
  \mathcal{L}(\mathbb{D},z) = \mathbb{E}_{\theta \sim Q} \left[\ln(P(\mathbb{D} \mid \theta;z)P(\theta)) - \ln(Q(\theta;z)) \right].
$$


## Robustness via Bayesian Learning 

::::{.columns}
:::{.column width=50%}

<br/>
<br/>

:::{.incremental}
* <span style="font-variant:small-caps;">NeuralPbc</span> assumes a nominal model $\dot{x} = f(x, u)$
* The trained controller must not overfit on the observations generated from the nominal model

:::
:::

:::{.column width=50%}

![](contents/assets/regular-vs-bayesian.svg){.absolute width="600" heigth="600"}

:::
<!-- column -->

::::

:::{.fragment fragment-index=0}
:::{.incremental}
* **Our method**: $H_d$ is a Bayesian neural network
    + achieves the performance objective $\ell$ for samples $\theta \sim Q(\theta; z)$
    + searches for ensemble of parameters that meet the desired performance 
:::
:::