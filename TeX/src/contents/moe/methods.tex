\section{Mixture of Expert Controllers}
\label{sec:moe_methods}

In this section, we present a data-driven control design framework for hybrid
dynamical systems. 
%
The objective of this framework is to learn a mixture of expert controllers and
their responsibilities as determined by the gating network.
%
This technique allows us to observe the effects of contacts from the closed loop
trajectories  and learn a switching mechanism to best control the hybrid system
in all modes.
%
We also learn optimal expert controllers that achieve a certain performance
objective through data-driven techniques.

%
Let $\phi(x_0, u, T)$ denote a closed loop trajectory with initial state $x_0$
integrated for the time horizon $T$.
%
For every state $x$ in the trajectory, the control law first samples state
partition (bin) number from the categorical distribution
in~\eqref{eq:gating_categorical}.
%
The control input at state $x$ is
\begin{align*}
    u(x; \psi, \theta) = \{F_i&(x; \theta_i) \; | \; i  \sim \text{Categorical} (\mathbf{P}(x| \psi)) \}
\end{align*} 
%
Without prior knowledge injected to the gating network, the samples from the
categorical distribution initially explore the performance of most, if not all,
of the expert controllers.
%
We use the running cost $\ell : \mathcal{X} \times \mathcal{U} \rightarrow
\mathbb{R}$ to measure the performance of the sampled experts, which we discuss
in depth in the next section.
%
The running cost plays the role of \it{prediction error} \normalfont in the
construction of the likelihood as shown in~\eqref{eq:log_normal_likelihood}.
%
The goal is to learn the decision parameters $(\psi, \theta)$ that minimize the
running cost for all initial states in the state space.
%
We pose the search over the parameters of the experts and the gating network as
\begin{equation}
    \begin{aligned}
        \underset{\psi, \theta}{\textrm{minimize}} 
        & & & \ell (x,u)  , \\%
        \textrm{subject to}
        & & M(q) &\dd \dot{q} + h(x; \psi, \theta)\dd t - \dd R  = 0,\\%
        & & u = \{F_i&(x; \theta_i) \; | \; i  \sim \text{Categorical} (\mathbf{P}(x| \psi)) \}
    \end{aligned}
    \label{eq:moe_opt}
\end{equation}
%
In the coming sections, we provide a procedure to solve the
optimization problem~\eqref{eq:moe_opt}. 

\subsubsection{Performance Objective}
\label{sssec:performance_objective}
%
We present two viable choices for the running cost function. 
\begin{enumerate}
    \item \textbf{Accumulated loss}: is the total quadratic loss between the desired
    state $x^*$ and the states generated under the current control law. We also
    incur a cost on the control authority as follows.
    \begin{equation}
        \begin{gathered}
            \ell(x, u) = \frac{1}{2}(x - x^*)^\top \mathcal{Q} (x - x^*) + \frac{1}{2} u^\top \mathcal{R} u , 
            % \ell(\phi, u) = \int_0^{T}  \ell(x(t), u)\dd t,
        \end{gathered}
    \label{eq:accumulatedLoss}
    \end{equation}
    \noindent where $\mathcal{Q} \succ 0$ is positive definite matrix and $\mathcal{R} \succeq 0$ is
    a positive semi-definite matrix.
    %
    This construction encourages trajectories to reach the desired equilibrium
    with minimum effort and shortest time.
    %

    We construct the corresponding likelihood as follows.
    %
    At each integration step, we evaluate the performance of each expert as
    shown in Algorithm~\ref{algo:accumulated_loss}.
    %
    The running cost of each expert is weighed by its \textit{responsibility} $P_i(x | \psi)$.
    The resulting likelihood is given by
    \begin{align}
        \mathbb{L}(\phi) = \int_{0}^{T} \Biggl( \sum_{i=1}^{N_F} \Bigl[ - \ell \Bigl(x(t), F_i \Bigr) P_i\Bigl(x(t) | \psi \Bigr) \Bigr] \Biggr) \dd t.
        \label{eq:accumulated_likelihood}
    \end{align}
    %
    % Notice that for each state $x(t)$, we need to test the performance of each expert. 
    % %
    % For instance, if there are $100$ states in a trajectory and $3$ experts, we need to evaluate $\ell$ $300$ times.

    \begin{algorithm}[H]
        \setstretch{1.2}
        \caption{Accumulated Loss}
        \label{algo:accumulated_loss}
        \small
        \hspace*{\algorithmicindent} \textbf{Input}: $x_0, \theta, \psi$
        \begin{algorithmic}[1]
            \State $\mathbb{L} \leftarrow 0$
            % \algrenewcommand\algorithmicindent{0em} % No indent
                \For{$t = 0:\Delta t:T$}     
                \For{$j=1:N_F$}\Comment{Evaluate performance of each expert}
                    \State $\hat{x} \leftarrow \texttt{Moreau's one time step}(x(t), F_j(x(t), \theta_j))$\Comment{Algorithm~\eqref{algo:moreau}}
                    \State $\mathbb{L} \leftarrow \mathbb{L} - \ell(\hat{x}, F_j)P_j(\hat{x} | \psi)$
                \EndFor
                \State $i \sim \text{Categorical}\Bigl(\mathbf{P}(x(t)| \psi)\Bigr)$ \Comment{Sample a bin number}
                \State $x(t + \Delta t) \leftarrow \texttt{Moreau's one time step}(x(t), F_i(x(t), \theta_i))$
                \EndFor
            \State \textbf{return} $\phi, \mathbb{L}$
        \end{algorithmic}
    \end{algorithm}
  
    \item \textbf{Minimum trajectory loss (MTL)}: While a trivial choice,
    accumulated loss may not reflect the desired behavior of some dynamical
    systems.
    %
    For instance, suppose we want to swing-up the simple pendulum to the upright
    equilibrium. 
    %
    For an underactuated pendulum, the controller needs to swing about the
    downward equilibrium, moving the states closer and further away from the
    upright.
    %
    Accumulated loss incurs a lot of cost in such scenarios and the control
    search would get stuck in local minima.
    %
    In such cases, a successful loss function encourages trajectories that
    \it{eventually} \normalfont lead to a minimum cost.
    %
    Hence, we compose MTL as
    \begin{equation}
        \begin{gathered}
            t_{min} = \underset{t}{\textrm{inf}} \; \{ \ell(x, u): x \in \phi(x_0, u, T) \}  \\
            \mathbb{L}(\phi) = - \frac{\ell(x(t_{min}), u)}{C} \sum_{t=0}^{t_{min}}P_i(x(t) | \psi) 
        \end{gathered} 
    \end{equation}
    \noindent where $C > 0$ is a normalization factor.
    %
    Unlike accumulated loss, MTL does not particularly reward low effort or
    short time trajectories, but it equally rewards two trajectories
    as long as they both reached the desired state within the time horizon $T$. 
    %
    Moreover, MTL does not need to evaluate the performance of each expert at
    every integration step, as shown in Algorithm~\eqref{algo:mtl}; this greatly
    reduces the computational cost.
    %
    
    \begin{algorithm}[H]
        \setstretch{1.2}
        \caption{Minimum Trajectory Loss}
        \label{algo:mtl}
        \small
        \hspace*{\algorithmicindent} \textbf{Input}: $x_0, \theta, \psi$
        \begin{algorithmic}[1]
            \State $\phi \leftarrow \{x_0\}$
            % \algrenewcommand\algorithmicindent{0em} % No indent
                \For{$t = 0:\Delta t:T$}     
                    \State $i \sim \text{Categorical}\Bigl(\mathbf{P}(x(t)| \psi)\Bigr)$ \Comment{Sample a bin number}
                    \State $x(t + \Delta t) \leftarrow \texttt{Moreau's one time step}(x(t), F_i(x(t), \theta_i))$\Comment{Algorithm~\eqref{algo:moreau}}
                    \State $\phi \leftarrow \phi \cup x(t + \Delta t)$\Comment{Save trajectory}
                \EndFor
                \State $t_{min} = \underset{t}{\textrm{inf}} \; \{ \ell(x, u): x \in \phi\}$
                \State $\mathbb{L} = - \ell(\phi(t_{min}), u) \sum_{t=0}^{t_{min}}P_i(\phi(t) | \psi)  $

            \State \textbf{return} $\phi, \mathbb{L}$
        \end{algorithmic}
    \end{algorithm}
\end{enumerate}

\subsubsection{State Sampling}
\label{sssec:state_sampling}

We intend to find a solution to the optimization problem
in~\eqref{eq:moe_opt} for all initial states $x_0$ in the
state space.
%
To efficiently sample the initial states, we use a combination of greedy and
explorative state sampling techniques.
%
Greedy state sampling, commonly known as \textsc{DAgger}, is a technique
adapted from imitation learning~\cite{ross2011no}.
%
This technique refines the controller on the states most visited under the
current parameters $(\psi, \theta)$.
%
We first sample several initial states randomly and generate trajectories using
the current parameters.
%
Then, we randomly select $N_d$ samples from the visited states. 
%
The explorative state sampling technique helps recover from locally optimal
solutions. 
%
This is achieved by sampling $N_r$ initial states around the neighborhood of the
desired state $x^*$.
%
In a single batch training, we compute the running cost as an expectation
over $N_{\mathcal{D}} = N_d+N_r$ samples as follows:
\begin{align*}
    J(\phi, u) = \mathbb{E}_{x_0 \sim \mathcal{D}_N}[ \mathbb{L}(\phi(x_0, u, T))]
\end{align*}
\noindent where $\mathcal{D}_N$ is a collection of $N_{\mathcal{D}}$ initial state samples.

\subsubsection{Training MOE}
\label{sssec:training_moe}

We solve the optimization problem given in~\eqref{eq:moe_opt} with stochastic
gradient descent (SGD).
%
We invoke a variant of SGD known as \textsc{Adam}~\cite{kingma2014adam} to
efficiently train the parameters with adaptive learning rates $\alpha_i$.
%
The full training procedure is outlined in Algorithm~\eqref{algo:moe_training}.
%
We leverage forward-mode auto-differentiation technique~\cite{revels2016forward}
to back-propagate on the gradient of the likelihood with respect to the learned
parameters.
\begin{algorithm}[tb]
    \setstretch{1.2}
      \caption{Solution to the Optimization Problem~\eqref{eq:moe_opt}}
      \label{algo:moe_training}
      \small
      \begin{algorithmic}[1]
          \algrenewcommand\algorithmicindent{0em} % No indent
          \State $\mathcal{D}_N \gets \{x_0\}_{(N_{\mathcal{D}})}$  \Comment{$N_{\mathcal{D}}$ initial state samples} 
          \algrenewcommand\algorithmicindent{1.1em} % Change indent back to default
          \While{$i < $ \texttt{maximum iteration}}
          \State $J \gets 0$\Comment{Batch loss}
          \For{$z_0 \in \mathcal {D}_N$}
              \State $\phi, \mathbb{L}$ = \texttt{Performance objective}($x_0, \psi, \theta$) \Comment{Algorithm~\eqref{algo:accumulated_loss} or~\eqref{algo:mtl}}
              \State $J \gets J + \mathbb{L}/N_{\mathcal{D}}$ 
          \EndFor
          \State $\theta \gets \theta + \alpha_i \nicefrac{\partial J}{\partial \theta}$\Comment{SGD step}
          \State $\psi \gets \psi + \alpha_i \nicefrac{\partial J}{\partial \psi}$
          \State $\mathcal{D}_N \gets \{x_0\}_{(N_{\mathcal{D}})}$\Comment{New initial state samples}
          \State $i \;\:\gets i + 1$
          \EndWhile
          \State \textbf{return} $\theta$
      \end{algorithmic}
  \end{algorithm}

\subsubsection{Back-propagation through Hybrid Systems}

The training framework outlined~\eqref{eq:moe_opt} allows us to observe the
effects of contacts in the closed loop trajectories and infer a controller that
either uses the contact to its advantage or minimizes its adverse effects.
%
In this section, we look at the relevant parts of the back-propagation to give
insight on how this is achieved.
%
We also show that despite the state jumps in the hybrid dynamics, the
derivatives involved in the back-propagation are well-defined.

Suppose we generate a short trajectory $\phi$ with the sampled expert control
parameter $\theta_i$.
%
Forward-mode auto-differentiation evaluates the gradient of the accumulated cost
with respect to $\theta_i$ as 
\begin{align*}
    \frac{\partial \ell}{\partial \theta_i} = \sum_{t=0}^{T} \frac{\partial \ell}{\partial x_t} \frac{\partial x_t}{\partial \theta_i} ,
\end{align*}
\noindent where $R=0$ for simplicity.
%
Without loss of generality, we take one integration step for the reminder of
this discussion. 
%
In that step, a contact event is triggered causing the velocities to jump
between the initial state $x_0$ and the following state $x_1$. 
%
Hence, we focus on 
\begin{align*}
    \frac{\partial \ell}{\partial \theta_i} = \frac{\partial \ell}{\partial x_1} \frac{\partial x_1}{\partial \theta_i},
\end{align*}
%
We can expand the gradient further as 
\begin{align*}
    \frac{\partial \ell}{\partial \theta_i} = \frac{\partial \ell}{\partial x_1} \Bigl( \frac{\partial x_1}{\partial u} \frac{\partial u}{\partial \theta_i} + \frac{\partial x_1}{\partial \lambda}\frac{\partial \lambda}{\partial \theta_i}\Bigr),
\end{align*}
\noindent where $\lambda$ holds the contact forces.
%
We can compute the first term from Moreau's integration step as
\begin{align*}
    \frac{\partial x_1}{\partial u} \frac{\partial u}{\partial \theta_i} = 
    \bmat{M^{-1}B \Delta t^2/2 \\ M^{-1}B \Delta t} \frac{\partial u}{\partial \theta_i},
\end{align*} 
%
At first glance, it may seem the derivative $\frac{\partial x_1}{\partial
\lambda}\frac{\partial \lambda}{\partial \theta_i}$ does not exist due to
the discontinuity in the states. 
%
A closer observation reveals that  $\frac{\partial x_1}{\partial \lambda}$
determines how the \it{post-impact velocity is affected by the
contact forces} \normalfont.
%
In fact, the derivative can be found from Moreau's integration as 
\begin{align*}
    \frac{\partial x_1}{\partial \lambda} = \bmat{W_N & W_T }, 
\end{align*}
%
demonstrating that the gradient is exists even if a state jump has
occurred. 
%
This term is crucial in adjusting the decision parameters in response to how the
contact force assists or inhibits the system.
%
If the contact forces affect the \it{post-impact} \normalfont velocity such that
the resulting generalized coordinates are closer to the desired state $x^*$,
then the gradient $\frac{\partial \ell}{\partial x_1} \frac{\partial
x_1}{\partial \lambda}\frac{\partial \lambda}{\partial \theta_i}$ adjusts the
parameter $\theta_i$ to favor states undergoing contact events. 
%
We in fact demonstrate this behavior in simulation and real-world experiments in
Section~\ref{ssec:cartpole_with_walls}.
%
Conversely, if the contact forces move the states further away from $x^*$, the
gradient leads to control parameters that attempt to recover from the outcomes
of the contact events.
%
We also demonstrate this behavior on a walking robot example in
Section~\ref{sssec:rimless_wheel_model}.