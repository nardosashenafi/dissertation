
\section{Background}

\subsection{Passivity-Based Control (PBC)}

Suppose we have a robotic system whose Hamiltonian $H$ can be expressed as
%
\begin{equation}
    H(q,p) = \frac{1}{2} p^\top M^{-1}(q) p + V(q),
    \label{eq:system_hamiltonian}
\end{equation}
%
where $p \in \mathbb{R}^m$ is the generalized momenta, and $V(q)$ represents the
potential energy. Hamilton's equations of motion are given by

\begin{align}
    \begin{split}  
      f(x, u) &= \bmat{\nabla_pH \\ -\nabla_qH}\ + \bmat{0 \\ \Omega(q)}u, \\
      &\hspace{-0.15cm} y = \Omega(q)^\top \dot{q},
    \end{split}
    \label{eq:hamiltonian_dynamics}
\end{align}
\noindent where $\Omega(q) \in \mathbb{R}^{m \times n}$ is the input matrix, and $u
\in \mathbb{R}^{n}$ is the control input.
%
% The system~\eqref{eq:hamiltonian_dynamics} is \textit{underactuated} if rank $\Omega
% = n < m$.
%
Passivity-based control leverages the stability properties of passive systems to
design a stable closed-loop system.
%
A mechanical system is considered passive if it is dissipative, i.e.
\begin{align}
  H(x(t_1)) \leq H(x(t_0)) + \int_{t_0}^{t_1} s(u(t), y(t)) dt,
\end{align}
\noindent for all initial state $x(t_0$) and all input $u$ under the supply rate
$s = u^\top y$.
%
From Lyapunov stability theory, the system~\eqref{eq:hamiltonian_dynamics} is
passive and therefore the origin $x = (0, 0)$ is stable because 
\begin{equation*}
  \begin{gathered}
    \dot{H} = \frac{\partial H}{\partial x} f(x, u) \leq u^\top y, \\
    H \geq 0.
  \end{gathered}
\end{equation*}

The objective of passivity-based control (PBC) is to design a control
law $u$ that imposes the desired storage function $H_d: \mathcal{X} \rightarrow
\mathbb{R}$ on the closed-loop system, rendering it passive and therefore
stable~\cite{van2000l2}.
%
The dynamics of the resulting closed-loop system is 
\begin{align}
  \begin{split}  
    f(x, u) &= \bmat{\nabla_pH_d \\ -\nabla_qH_d}\, \\
  \end{split}
  \label{eq:desired_hamiltonian_dynamics}
\end{align}
\noindent and it has a new desired stable equilibrium at $x^*$.

From~\eqref{eq:hamiltonian_dynamics}
and~\eqref{eq:desired_hamiltonian_dynamics}, we can find the energy shaping term
that creates a passive closed-loop system as
%
\begin{equation}
  u_{es}(x) =  -\Omega^{\dagger} \left( \nabla_q H_d - \nabla_q H \right).
  \label{eq:esc}
\end{equation}
\noindent where $\Omega^\dagger = \left( \Omega^\top \Omega  \right)^{-1}
\Omega^\top$. We also introduce a damping term $u_{di}$ that results in an
asymptotically stable system. The resulting controller is
\begin{align}
  \begin{split} 
    u &= u_{es}(x) + u_{di}(x), \\
    &u_{di}(x) = - K_{v} \, y
  \end{split}
  \label{eq:damping_and_es_control}
\end{align}
\noindent where $K_v \succ 0$ is the damping gain matrix.
From~\eqref{eq:damping_and_es_control}, we can construct a constraint on the
form of $H_d$ as 
\begin{align}
  \Omega^\bot \left( \nabla_q H_d - \nabla_q H \right) = 0,
    \label{eq:pdes}
\end{align}
where $\Omega^\perp \Omega = 0$ and $H_d$ has a minimum at the desired
equilibrium $(q^\star, p^*)$. 
%
However, the closed-form solution to the partial differential
equations (PDEs) in~\eqref{eq:pdes} is intractable. 

\subsubsection{Neural PBC}

The deterministic \textsc{NeuralPbc} framework presented in~\cite{neuralpbc}
solves the PDEs~\eqref{eq:pdes} by rewriting the PBC problem as the following 
optimization problem
\begin{equation}
  \begin{aligned}
      \underset{\theta}{\textrm{minimize}} 
      & & &\int_{0}^{T} \ell \left(\phi,u^{\theta}(\phi) \right) \, \dd t , \\%
      \textrm{subject to}
      & & f(x, u) &= \bmat{\phantom{-}\nabla_p H \\ -\nabla_q H} + \bmat{0 \\ \Omega(q)}u^{\theta}, \\%
      & & u^{\theta} &= -\Omega^{\dagger} \nabla_q H_d^{\theta} - K_v^{\theta} \Omega^\top \nabla_p H_d^{\theta},%
      % & \quad x(0) &= x_0 \in \mathcal{X}, \\%
  \end{aligned}
  \label{eq:neural_pbc_finite_optim}
\end{equation}
where $T>0$ is the time horizon, $\ell: \mathcal{X} \times \mathcal{U}
\rightarrow \mathbb{R}$ is a running cost function to be defined, and $\phi(
x_0, u^\theta, T)$ is a closed-loop trajectory generated from the initial state
$x_0$ under the current control law $u^\theta$.
%
The \textsc{NeuralPbc} technique adds three important features to the classical 
PBC framework.
\begin{enumerate}
  \item The optimization problem finds an approximate solution to the PDEs
  in~\eqref{eq:pdes} using stochastic gradient descent.
  \item Desired system behavior is explicitly introduced into the optimization
  via the performance objective $\ell$.
  \item The framework leverages the universal approximation capabilities of
  neural networks to parameterize the desired Hamiltonian $H^\theta_d$.
\end{enumerate}

\subsubsection{Neural Interconnection and Damping Assignment PBC}

\textsc{IdaPbc}, a variant of \textsc{Pbc}, selects a particular structure for
$H_d$ 
\begin{equation}
  H_d(q, p) = \frac{1}{2} p^\top M_d^{-1}(q) p + V_d(q),
  \label{eq:idapbc_desired_hamiltonian}
\end{equation}
such that the control input must satisfy the PDEs given by 
\begin{equation}
  G^\perp \left\{ \nabla_qH - M_dM^{-1} \nabla_qH_d + J_2M_d^{-1}p \right\} = 0.
  \label{eq:pde_idapbc}
\end{equation}
The objective is to learn $V_d$ and the entries of $M_d$ and $J_2$ matrices.
Once $V_d$, $M_d$ and $J_2$ are obtained, the energy-shaping control and the
damping injection term are given by
%
\begin{align}
  \begin{split}
  u_{es} &= G^{\dagger} \left(\nabla_qH - M_dM^{-1} \nabla_qH_d + J_2M_d^{-1}p\right), \\
  u_{di} &= -K_v G^\top \nabla_p H_d,
  \end{split}
  \label{eq:idapbc_ues}
\end{align}
%
respectively, where $G^{\dagger} = \left(G^\top G\right)^{-1} G^\top$.

The closed-form solution to the PDEs in~\eqref{eq:pde_idapbc} is intractable.
Hence, the deterministic \textsc{Neural-IdaPbc} framework introduced
in~\cite{neuralidapbc} formulates the following optimization problem that finds
an approximate solution to the PDEs.

\begin{equation}
  \begin{aligned}
      \underset{\theta }{\textrm{minimize}} 
      &&\quad \left\| l_{\textrm{IDA}} (x) \right\|^2 &= \left\| G^\perp \left\{ \nabla_qH - M_dM^{-1} \nabla_qH_d + J_2M_d^{-1}p \right\} \right\|^2, \\
      \textrm{subject to} 
      &&\quad M_d^\theta &= \big( M_d^\theta \big)^\top \succ 0, \\
      &&\quad J_2^\theta &= -\big( J_2^\theta \big)^\top, \\
      % &&\quad q^\star &= \underset{q}{\textrm{argmin}} \; V_d^\theta.  \\
      &&\quad q^\star &= \underset{q}{\textrm{argmin}}\; V_d^\theta.
  \end{aligned}    
  \label{eq:idapbc_finite_optim}%
\end{equation}
where $V^\theta_d$ and the entries of the $M^\theta_d$ and $J^\theta_2$ matrices
are parameterized by neural networks. 


\subsection{Bayesian Learning}

The objective of Bayesian learning is to determine a stochastic model (target
function) that best fits observed data $\mathcal{D}$ with inherent noise. Let
this stochastic target function be represented by $F(x; \theta) : \mathcal{X}
\rightarrow \mathbb{R}^t$, where $\theta \in \Theta \subset \mathcal{R}^v$ is a
multivariate random variable that parameterizes the model. Given prior belief on
the distribution of the parameters $p(\theta)$, Bayesian learning finds a
posterior distribution $p(\theta \mid \mathcal{D})$ over $\theta$ that maximizes
the likelihood of the target function generating the dataset
$\mathcal{D}$~\cite{bishop2006pattern}. This can be expressed in terms of Bayes'
theorem as 
\begin{align}
  p(\theta | \mathcal{D}) = \frac{p(\mathcal{D} \mid \theta) p(\theta)}{p(\mathcal{D})}
  = \frac{p(\mathcal{D} \mid \theta) p(\theta)}{\int_\theta p(\mathcal{D} \mid \theta') p(\theta') d\theta'},
\end{align}
where $p(\mathcal{D} \mid \theta)$ is the likelihood function and
$p(\mathcal{D})$ is the evidence. While the likelihood and prior distribution
can be expressed explicitly, the evidence is intractable. This calls for
techniques that approximate or find the exact posterior distribution, some of
which are discussed in the following section.

\subsubsection{Posterior Distribution}

Bayesian learning provides various techniques to infer the posterior distribution
over the parameters $\theta$. Two of the most famous techniques are discussed as follows.
\begin{enumerate}
  \item Markov Chain Monte Carlo (MCMC) methods: learn the exact posterior
  distribution by collecting samples of $\theta$ either through random walk
  (e.g. Metropolis-Hastings) or following the gradient of the likelihood (e.g.
  Hamiltonian Monte Carlo). Metropolis-Hastings methods collect samples of
  $\theta$ from a conditional probability distribution until the samples
  converge to an equilibrium distribution per the properties of irreducible and
  aperiodic Markov chains~\cite{gilks1995markov}. Hamiltonian Monte Carlo (HMC)
  method, shown in detail in Algorithm~\ref{algo:hmc}, also finds the
  equilibrium distribution of the Markov chain, but unlike Metropolis-Hastings,
  it efficiently searches the parameter space through the gradient of the
  likelihood. In the case of HMC, the Markov chain is generated from two
  first-order differential equations shown in lines 5-7 of
  Algorithm~\ref{algo:hmc}. While HMC method learns the exact posterior
  distribution, it has slow convergence properties for high-dimensional
  parameters. In such cases, techniques such as variational inference compromise
  accuracy of the posterior distribution for speed of convergence.
  
%   \begin{algorithm}[H]
%       \centering
%       \caption{Gibb's Sampling}\label{algo:gibbs}
%       \begin{algorithmic}[1]
%           \State Select initial state $\theta = \{\theta_i: i = 1, \dots ,\theta_v\}$
%           \State $\tilde{p}(\theta | \mathcal{D}) = p(\mathcal{D} | \theta) p(\theta)$ \Comment{Compute posterior upto a normalization constant}
%           \State Select a transition probability of the Markov Chain $p(\theta_i | \theta_{\backslash i})$
%           \For{$t = 1, \dots, T$}
%           \State Sample $\theta_1^{t+1} \sim p(\theta_1 | \theta_2^{t}, \theta_3^{t}, \dots, \theta_v^{t})$
%           \State Sample $\theta_2^{t+1} \sim p(\theta_1 | \theta_1^{t+1}, \theta_3^{t}, \dots, \theta_v^{t})$
%           \State Sample $\theta_3^{t+1} \sim p(\theta_1 | \theta_1^{t+1}, \theta_2^{t+1}, \dots, \theta_v^{t})$ 
%           \State Sample $\theta_v^{t+1} \sim p(\theta_1 | \theta_1^{t+1}, \dots, \theta_{v-1}^{t})$ 
%           \EndFor
%           \State \textbf{Return}  $\theta^{T}$
%       \end{algorithmic}
% \end{algorithm}
\begin{algorithm}[tb]
    \centering
    \setstretch{1.5}
    \caption{Hamiltonian Monte Carlo}\label{algo:hmc}
    \begin{algorithmic}[1]
      \State Select initial state $\theta$ and momentum $r$ from prior knowledge
      \State Select regularization coefficient $\lambda$
      \State Create a set $\Theta$ to collect samples of $\theta$
      % \State Define the prior distribution with distribution parameters $\theta_0$ 
      \State Define $p(\theta, \mathcal{D}) \propto \exp(-E(\theta, \mathcal{D}))$, \; $E(\theta, \mathcal{D}) = \sum_{d \in \mathcal{D}}\vectornorm{F(x;\theta) - d}^2 + \lambda \vectornorm{\theta}^2$
      \For{$t = 0:\Delta t: T$}
      \State $r(t + \Delta t/2) = r(t) - \dfrac{\Delta t}{2} \dfrac{\partial E}{\partial \theta_i}(\theta(t), \mathcal{D})$
      \State $\theta(t + \Delta t) = \theta(t) + \Delta t \; r(t + \Delta t/2) $ 
      \State $r(t + \Delta t) = r(t + \Delta t/2) - \dfrac{\Delta t}{2}\dfrac{\partial E}{\partial \theta_i}(\theta(t + \Delta t), \mathcal{D})$
      \State $\nu \sim \text{Uniform}[0, 1]$
      \If {$E(\theta(t + \Delta t), \mathcal{D}) < E(\theta(t), \mathcal{D})$}
      \State $\Theta$ \leftarrow \; $\Theta \cup \theta(t + \Delta t)$ 
      \ElsIf{$\nu < \exp( E(\theta(t), \mathcal{D}) - E(\theta(t+\Delta t), \mathcal{D})) $}
      \State $\Theta$ \leftarrow \; $\Theta \cup \theta(t + \Delta t)$ 
      \ElsIf{$\nu > \exp( E(\theta(t), \mathcal{D}) - E(\theta(t+\Delta t), \mathcal{D})) $}
      \State Reject $\theta(t + \Delta t)$ 
      \EndIf
      \EndFor
      \State \textbf{Return} $\Theta$
    \end{algorithmic}
\end{algorithm}

%
\item Variational Inference (VI): this technique selects a posterior
distribution $q(\theta;z)$ from the conjugate families of the likelihood and
prior distributions. The goal is to learn the distribution parameters $z$ that
minimize the Kullback-Leibler divergence or equivalently maximize the evidence
lower bound (\textsc{Elbo}). The \textsc{Elbo}, $\mathcal{L}$, is given
by~\cite{cohen2016bayesian}
\begin{align}
  \begin{split}
  \mathcal{L}(\mathcal{D},z) &= \mathbb{E}_{\theta \sim q} \left[\log(p(\theta, \mathcal{D};z)) - \log(q(\theta;z)) \right], \\
  p(\theta, \mathcal{D};z) &= p(\mathcal{D} \mid \theta;z)p(\theta),
  \end{split}
  \label{eq:elbo}
\end{align}
where $p(\mathcal{D} \mid \theta;z)$ is the likelihood function. 
\end{enumerate}
\begin{rem}
  For continuous posterior distribution, the \textsc{Elbo} given in
  equation~\eqref{eq:elbo} is redefined using differential entropy, which
  expresses the prior and posterior in terms of their probability density
  functions. In this case, the likelihood $p(\theta \mid \mathcal{D};z)$ is also
  a probability density function and the \textsc{Elbo} is not bounded by zero.
\end{rem}

The power of Bayesian learning lies in its ability to build a target function
and make predictions that integrate over
uncertainties~\cite{tipping2003bayesian}. These predictions can be found by
marginalizing the model over the posterior as follows~\cite{jospin2020hands}.
\begin{equation}
  \hat{F}(x) = \frac{1}{N} \sum_{\theta \sim q} F(x, \theta),
  \label{eqn:marginalization}
\end{equation} 
where $N$ is the number of samples drawn from the posterior. Moreover, Bayesian
frameworks can quantify the confidence in the predictions through the variance
of the predictive distribution, $p(F \mid x, \mathcal{D})$. The variance of
$p(F|x, \mathcal{D})$ is given by~\cite{jospin2020hands}
\begin{equation}
  \Sigma_{F \mid x,\mathcal{D}} = \frac{1}{N-1} \sum_{\theta \sim q} \vectornorm{F(x,\theta) - \frac{1}{N} \sum_{\theta \sim q} F(x, \theta)}^2.
  \label{eqn:predictive_variance}
\end{equation}
%
