
\section{Background}

In this section, we provide a brief summary to passivity-based control and the
recent data-driven variant introduced in~\cite{neuralpbc, neuralidapbc}.
%
We also motivate the advantages of the Bayesian learning framework in
uncertainty modeling and robust control.
\subsection{Passivity-Based Control (PBC)}
\label{ssec:pbc}

Suppose we have a robotic system whose Hamiltonian $H$ can be expressed as
%
\begin{equation}
    H(q,p) = \frac{1}{2} p^\top M^{-1}(q) p + V(q),
    \label{eq:system_hamiltonian}
\end{equation}
%
where $p \in \mathbb{R}^m$ is the generalized momenta, and $V(q)$ represents the
potential energy. Hamilton's equations of motion are given by

\begin{align}
    \begin{split}  
      f(x, u) &= \bmat{\nabla_pH \\ -\nabla_qH}\ + \bmat{0 \\ \Omega(q)}u, \\
      &\hspace{-0.15cm} y = \Omega(q)^\top \dot{q},
    \end{split}
    \label{eq:hamiltonian_dynamics}
\end{align}
\noindent where $x = (q, \dot{q})$, $\Omega(q) \in \mathbb{R}^{m \times n}$ is
the input matrix, and $u \in \mathbb{R}^{n}$ is the control input.
%
% The system~\eqref{eq:hamiltonian_dynamics} is \textit{underactuated} if rank $\Omega
% = n < m$.
%
Passivity-based control leverages the stability properties of passive systems to
design a stable closed-loop system.
%
A mechanical system is considered passive if it is dissipative, i.e.
\begin{align}
  H(x(t_1)) \leq H(x(t_0)) + \int_{t_0}^{t_1} s(u(t), y(t)) dt,
\end{align}
\noindent for all initial state $x(t_0$) and all input $u$ under the
supply rate $s = u^\top y$.
%
From Lyapunov stability theory, the system~\eqref{eq:hamiltonian_dynamics} is
passive and therefore the origin $x = (0, 0)$ is stable because 
\begin{equation*}
  \begin{gathered}
    \dot{H} = \frac{\partial H}{\partial x} f(x, u) \leq u^\top y, \\
    H \geq 0.
  \end{gathered}
\end{equation*}

The objective of passivity-based control (PBC) is to design a control law $u$
that imposes the desired storage function $H_d$ on the closed-loop system,
rendering it passive and therefore stable~\cite{van2000l2}.
%
The dynamics of the resulting closed-loop system is 
\begin{align}
  \begin{split}  
    f(x, u) &= \bmat{\nabla_pH_d \\ -\nabla_qH_d}\, \\
  \end{split}
  \label{eq:desired_hamiltonian_dynamics}
\end{align}
\noindent and it has a new desired stable equilibrium at $x^*$.

From~\eqref{eq:hamiltonian_dynamics}
and~\eqref{eq:desired_hamiltonian_dynamics}, we can find the energy shaping term
that creates a passive closed-loop system as
%
\begin{equation}
  u_{es}(x) =  -\Omega^{\dagger} \left( \nabla_q H_d - \nabla_q H \right).
  \label{eq:esc}
\end{equation}
\noindent where $\Omega^\dagger = \left( \Omega^\top \Omega  \right)^{-1}
\Omega^\top$. We also introduce a damping term $u_{di}$ that results in an
asymptotically stable system. The resulting controller is
\begin{align}
  \begin{split} 
    u &= u_{es}(x) + u_{di}(x), \\
    &u_{di}(x) = - K_{v} \, y
  \end{split}
  \label{eq:damping_and_es_control}
\end{align}
\noindent where $K_v \succ 0$ is the damping gain matrix.
From~\eqref{eq:damping_and_es_control}, we can construct a constraint on the
form of $H_d$ as 
\begin{align}
  \Omega^\bot \left( \nabla_q H_d - \nabla_q H \right) = 0,
    \label{eq:pdes}
\end{align}
where $\Omega^\perp \Omega = 0$ and $H_d$ has a minimum at the desired
equilibrium $x^* = (q^\star, p^*)$. 
%
However, for high-dimensional systems, the closed-form solution to the partial
differential equations (PDEs) in~\eqref{eq:pdes} may be intractable. 

\subsubsection{Neural PBC}

The deterministic \textsc{NeuralPbc} framework presented in~\cite{neuralpbc}
solves the PDEs~\eqref{eq:pdes} by rewriting the PBC problem as the following 
optimization scheme:
\begin{equation}
  \begin{aligned}
      \underset{\theta}{\textrm{minimize}} 
      & & &\int_{0}^{T} \ell \left(\phi,u^{\theta}(\phi) \right) \, \dd t , \\%
      \textrm{subject to}
      & & f(x, u) &= \bmat{\phantom{-}\nabla_p H \\ -\nabla_q H} + \bmat{0 \\ \Omega(q)}u^{\theta}, \\%
      & & u^{\theta} &= -\Omega^{\dagger} \nabla_q H_d^{\theta} - K_v^{\theta} \Omega^\top \nabla_p H_d^{\theta},%
      % & \quad x(0) &= x_0 \in \mathcal{X}, \\%
  \end{aligned}
  \label{eq:neural_pbc_finite_optim}
\end{equation}
where $T>0$ is the time horizon, $\phi( x_0, u^\theta, T)$ is a closed-loop
trajectory generated from the initial state $x_0$ under the current control law
$u^\theta$ and $\ell$ is a running cost function that parameterizes the
performance of the current control.
%
The \textsc{NeuralPbc} technique adds three important features to the classical 
PBC framework.
\begin{enumerate}
  \item The optimization problem finds an approximate solution to the PDEs
  in~\eqref{eq:pdes} using stochastic gradient descent.
  \item Desired system behavior is explicitly introduced into the optimization
  via the performance objective $\ell$.
  \item The framework leverages the universal approximation capabilities of
  neural networks to parameterize the desired Hamiltonian $H^\theta_d$.
\end{enumerate}

\subsubsection{Neural Interconnection and Damping Assignment PBC}

\textsc{IdaPbc}, a variant of \textsc{Pbc}, selects a particular structure for
$H_d$ 
\begin{equation}
  H_d(q, p) = \frac{1}{2} p^\top M_d^{-1}(q) p + V_d(q),
  \label{eq:idapbc_desired_hamiltonian}
\end{equation}
\noindent where the minimum of the closed-loop potential energy $V_d(q)$ is at
the desired equilibrium coordinate $q^*$.
%
The resulting passive closed loop system is given by~\cite{ortega2002stabilization}
\begin{equation}
  \bmat{\dot{q} \\ \dot{p}}  =
  \bmat{0 & M^{-1}M_d \\ -M_dM^{-1} & J_2(q,p) - \Omega K_v \Omega^\top}
  \bmat{\nabla_q H_d \\ \nabla_p H_d},
  \label{eq:pch}
\end{equation}
where $J_2 = -J_2^\top$ and $M_d \succ 0$ is a positive-definite matrix.
%
We set the closed loop systems in~\eqref{eq:hamiltonian_dynamics}
and~\eqref{eq:pch} equal to each other to find the energy-shaping and the damping control terms as
\begin{align}
  \begin{split}
  u_{es} &= \Omega^{\dagger} \left(\nabla_qH - M_dM^{-1} \nabla_qH_d + J_2M_d^{-1}p\right), \\
  u_{di} &= -K_v \Omega^\top \nabla_p H_d,
  \end{split}
  \label{eq:idapbc_ues}
\end{align}
%
We build a constraint from~\eqref{eq:idapbc_ues} to extract $V_d$ and the
matrices $M_d$ and $J_2$. The resulting PDEs are given by 
\begin{equation}
  \Omega^\perp \left\{ \nabla_qH - M_dM^{-1} \nabla_qH_d + J_2M_d^{-1}p \right\} = 0.
  \label{eq:pde_idapbc}
\end{equation}
%
Similar to \textsc{NeuralPbc}, the closed-form solution to the PDEs
in~\eqref{eq:pde_idapbc} may be intractable. 

The deterministic \textsc{Neural-IdaPbc} framework introduced
in~\cite{neuralidapbc} finds an approximate solution to the PDEs from the
following optimization problem.
\begin{equation}
  \begin{aligned}
      \underset{\theta }{\textrm{minimize}} 
      &&\quad \left\| l_{\textrm{IDA}} (x) \right\|^2 &= \left\| \Omega^\perp \left\{ \nabla_qH - M_dM^{-1} \nabla_qH_d + J_2M_d^{-1}p \right\} \right\|^2, \\
      \textrm{subject to} 
      &&\quad M_d^\theta &= \big( M_d^\theta \big)^\top \succ 0, \\
      &&\quad J_2^\theta &= -\big( J_2^\theta \big)^\top, \\
      % &&\quad q^\star &= \underset{q}{\textrm{argmin}} \; V_d^\theta.  \\
      &&\quad q^\star &= \underset{q}{\textrm{argmin}}\; V_d^\theta,
  \end{aligned}    
  \label{eq:idapbc_finite_optim}%
\end{equation}
where $V^\theta_d$ and the entries of the $M^\theta_d$ and $J^\theta_2$ matrices
are parameterized by neural networks. 
%
To enforce the constraints shown in~\eqref{eq:idapbc_finite_optim}, we redefine
$M^\theta_d, J^\theta_2$ and $V^\theta_d$ as follows.
%
We constrain the positive-definiteness of $M^\theta_d$ with the Cholesky
decomposition 
\begin{align*}
  M^\theta_d = L_{\theta}(q)L_{\theta}^\top(q) + \delta_M I_n,
\end{align*}
\noindent where $\delta_M > 0$ is a small constant and $I_n$ is the $n \times n$
identity matrix.
%
The matrix $L_{\theta} \in \mathbb{R}^{n \times n}$ is a lower-triangular matrix
whose $\nicefrac{n(n+1)}{2}$ entries are outputs of a neural network. 
%
The skew-symmetrix matrix $J_2$ is constructed as 
\begin{align*}
  J_2^\theta(q, p) = A_\theta(q, p) - A^\top_\theta(q, p)
\end{align*}
\noindent where the entries of $A_{\theta}(q, p)$ are given by neural nets.
%
Lastly, we design a fully-connected neural network for $V^\theta_d$ such that it
has a minimum at $q^*=0$ as follows.
%
Let $V^\theta_d$ be a deep neural net with $j$ layers and all bias terms set to
zero. We denote this with
\begin{equation}
  V_d^\theta(x) = \Phi \Bigl( W_j\sigma(W_{j-1}\sigma(\ldots W_2\sigma(W_1x))) \Bigr),
  \label{eq:Vdnet_constraint}
\end{equation}
\noindent where $W_i$ holds the weights of layer $i$ and $\sigma$ is the activation function.
%
The activation function is chosen such that $\sigma(0) = 0$, ensuring that
$\Phi(0) = 0$ and $\Phi(x) > 0, x \ne 0$.
%
Several choices for the activation function include \textsc{Elu}, \textsc{Tanh} and \textsc{Relu}.
%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bayesian Learning}
\label{ssec:bayesianLearning}

Suppose we are given a finite dataset from an unknown noise process, for which
we are trying to fit a regression model.
%
The goal is not only to predict the average outputs of the source, but also
the uncertainty of the noise process.
%
Bayesian learning uses the expressive power of stochastic models to best
represent the data source and report the uncertainty in
predictions~\cite{bishop2006pattern}.


Let $F(x; \theta)$ denote the stochastic model whose parameters $\theta$ are
multivariate random variables.
%
These parameters are samples drawn from a posterior probability distribution.
%
The objective is to find the posterior distribution that minimizes the quadratic
prediction error $\sum_{(x_j, y_j) \in \mathbb{D}} \norm{F(x_j; \theta) -
y_j}{}$, where $\mathbb{D} = \{(x_1, y_1), \dots, (x_N, y_N) \}$ the training
dataset with inputs $x_j$ and observations $y_j$.
%
As discussed in Section~\ref{ssec:mixture_of_experts}, we can use
\it{expectation maximization} \normalfont (EM) to learn the optimal posterior
distribution.
%
To do so, we define the likelihood in terms of the quadratic prediction error as
\begin{equation*}
  P(\mathbb{D} | \theta)  = \prod_{j=1}^{N} \mathcal{N}(\norm{F(x_j; \theta) -  y_j}{}, s),
\end{equation*}
\noindent for independent observations in the dataset $\mathbb{D}$.
Even though the EM approach finds optimal parameters $\theta$ that
minimize the prediction error, this technique is prone to overfitting.
%
Given finite number of observations, EM finds low variance posterior
distribution \it{biased to the training
data}\normalfont~\cite{bishop2006pattern}. 
%
The risk of overfitting is especially an issue when we are modelling the noise
in the data source, which requires some amount of variance.
%
The solution to this problem involves finding a \it{bias-variance
trade-off}\normalfont, where the training adjusts the parameters based on the
likelihood, but also enforces the posterior to hold some variance to prevent
overfitting.
%
% We can observe the importance of bias-variance tradeoff directly from the
% prediction error as follows.
% %
% Let us add and subtract the average prediction $\mathbb{E}_{\mathbb{D}}(F(x))$
% from the prediction error, where $\mathbb{D}$ is a collection of datasets $Y_x$. We get
% \begin{align*}
%   (F(x) - Y_x)^2 &= (F(x) - \mathbb{E}_{\mathbb{D}}[F(x)] + \mathbb{E}_{\mathbb{D}}[F(x)] - Y_x)^2 \\
%   &= (F(x) - \mathbb{E}_{\mathbb{D}}[F(x)])^2 + (\mathbb{E}_{\mathbb{D}}[F(x)] - Y_x)^2 + 2(F(x) - \mathbb{E}_{\mathbb{D}}[F(x)])(\mathbb{E}_{\mathbb{D}}[F(x)] - Y_x)
% \end{align*}
% The prediction error over the entire dataset $\mathbb{D}$ is given by the expectation
% \begin{align*}
%   \mathbb{E}_{\mathbb{D}}[(F(x) - Y_{x})^2] = (\mathbb{E}_{\mathbb{D}}[F(x)] - Y_x)^2 + \mathbb{E}_{\mathbb{D}}[\{F(x) - \mathbb{E}_{\mathbb{D}}(F(x)) \}^2]
% \end{align*}
%
Bias-variance trade-off is achieved with the introduction of a prior
distribution, which adds a regularization term to the
likelihood~\cite{bishop2006pattern}.
%
In supervised learning, regularization terms restrict the training from learning
a complex model, minimizing the risk of overfitting~\cite{santos2022avoiding}.
%
Similarly, the prior distribution prevents the learned parameters from becoming
\it{overconfident} \normalfont in their predictions.
%
In this construction, the posterior distribution $P(\theta | \mathbb{D})$ is
defined in terms of the likelihood and prior with Bayes' theorem as 
\begin{align}
  P(\theta | \mathbb{D}) = \frac{P(\mathbb{D} | \theta) P(\theta) }{P(\mathbb{D})}
  = \frac{P(\mathbb{D} | \theta) P(\theta) }{\int_\theta P(\mathbb{D} | \theta') P(\theta')  d\theta'}, 
  \label{eq:bayes_posterior}
\end{align}
where  $P(\theta)$ is the prior and $P(\mathbb{D})$ is the evidence or the
normalization constant of the posterior.
%
The relative weighting between the likelihood and the prior is parameterized by
the standard deviation $s$ of the likelihood. The higher the standard deviation,
the more weight we give to the regularization enforced by the prior.
%
The rate at which we update the prior distribution also determines the
regularization weight.
%
The posterior in~\eqref{eq:bayes_posterior} shows that the likelihood and the
prior are in a tug of war. The likelihood pulls the parameters towards
minimizing the prediction error, but the prior distribution gives priority to the
initial distribution of the decision parameters.
%
This tug of war prevents the posterior from finding a near zero-variance
solution, and consequently achieving the bias-variance trade-off.

\subsubsection{Posterior Distribution}

While the likelihood and prior distributions in~\eqref{eq:bayes_posterior} can
be expressed explicitly, the evidence is intractable. We leverage Bayesian
inference techniques to approximate or find the exact posterior distribution
without the use of the evidence. Two of the most famous techniques are
discussed as follows.
\begin{enumerate}
  \item \textbf{Markov Chain Monte Carlo (MCMC) methods}: find the exact
  posterior distribution from a collection of samples of $\theta$.
  %
  MCMC methods collect these samples from a proposal distribution
  $\tilde{Q}(\theta^{(\tau)} | \theta^{(\tau-1)})$, where the sequence of
  samples $\theta^\tau$ form a Markov Chain~\cite{bishop2006pattern}.
  %
  The proposal distribution is known up to its normalization constant and is
  sufficiently simple to sample from. 
  %
  We accept or reject each candidate sample according to the following rule~\cite{bishop2006pattern}:
  \begin{align*}
    \nu &\sim \mathcal{U}(0, 1), \\
    A(\theta^{(\tau)}, \theta^{(\tau-1)}) &= \min \Biggl(1, \frac{P(\mathbb{D} | \theta^{(\tau)})P(\theta^{(\tau)})}{P(\mathbb{D} | \theta^{(\tau-1)})P(\theta^{(\tau-1)})} \Biggr),
  \end{align*}
  \noindent where $\mathcal{U}$ is the uniform distribution. If
  $A(\theta^{(\tau)}, \theta^{(\tau-1)}) \geq \nu$, then we accept the sample. 
  %
  Otherwise, we discard the candidate and resample from $\tilde{Q}(\theta^{(\tau)} |
  \theta^{(\tau-1)})$.
  %
  MCMC methods such as Metropolis-Hastings collect the next sample through
  random walk~\cite{gilks1995markov}, while other gradient-based techniques such
  as Hamiltonian Monte Carlo (HMC) method, efficiently search the
  parameter space through the gradient of the likelihood.
  %
  Even though the MCMC methods learn the exact posterior distribution, they have
  slow convergence properties for high-dimensional parameters. In such cases,
  techniques such as variational inference compromise accuracy of the posterior
  distribution for speed of convergence.
  
\item \textbf{Variational Inference (VI)}: is a gradient-based technique that approximates the posterior with the pre-selected
distribution $Q(\theta;z)$.
%
The approximate posterior is selected from the conjugate families of the likelihood and
prior distributions. The goal is to learn the distribution parameters $z$ of the approximate posterior that
minimize the Kullback-Leibler divergence or equivalently maximize the evidence
lower bound (\textsc{Elbo}). The \textsc{Elbo}, $\mathcal{L}$, is given
by~\cite{cohen2016bayesian}
\begin{align}
  \begin{split}
  \mathcal{L}(\mathbb{D},z) &= \mathbb{E}_{\theta \sim Q} \left[\log(P(\mathbb{D} \mid \theta;z)P(\theta)) - \log(Q(\theta;z)) \right].
  \end{split}
  \label{eq:elbo}
\end{align}
\end{enumerate}

\begin{rem}
  For continuous posterior distribution, the \textsc{Elbo} given in
  equation~\eqref{eq:elbo} is redefined using differential entropy, which
  expresses the prior and posterior in terms of their probability density
  functions. In this case, the likelihood $P(\theta \mid \mathbb{D};z)$ is also
  a probability density function and the \textsc{Elbo} is not bounded by zero.
\end{rem}

% \subsubsection{Prediction}

Once we find the exact or approximate posterior, the prediction for state $x$
can be found in one of two ways. The first option is to marginalize the model
over the posterior as follows~\cite{jospin2020hands}
\begin{equation}
  \hat{F}(x) = \frac{1}{N_Q} \sum_{\theta \sim Q} F(x; \theta),
  \label{eqn:marginalization}
\end{equation} 
where $N_Q$ is the number of samples drawn from the posterior. 
%
The second option only takes one sample from the posterior, and it corresponds
to the maximum aposteriori (MAP), i.e. 
\begin{align} 
  \theta_{MAP}=\underset{\theta}{\textrm{argmax}}\; P(\theta | \mathbb{D})
\end{align}
Bayesian frameworks can quantify the confidence in the
predictions through the variance~\cite{jospin2020hands}
\begin{equation}
  \Sigma_{F \mid x,\mathbb{D}} = \frac{1}{N_Q-1} \sum_{\theta \sim Q} \vectornorm{F(x; \theta) - \frac{1}{N_Q} \sum_{\theta \sim Q} F(x; \theta)}^2.
  \label{eqn:predictive_variance}
\end{equation}
%
