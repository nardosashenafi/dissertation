

Many robotics applications involve hybrid systems that exhibit both continuous and 
discrete state transitions. Common examples of hybrid systems are contact-rich mechanisms such as legged robots and manipulators.
These systems constantly experience contact forces from their interaction with the environment, causing them to exhibit mode changes.
Controlling contact-rich robots comes with two main complications.
First, it is difficult to find a controller that can achieve the desired performance in all modes of the hybrid system.
%
One solution to this problem is to manually switch between several controllers by using some
event trigger that implies mode changes in the dynamics.
%
For instance, fuzzy control~\cite{kahraman2020fuzzy, katic2003survey} is commonly used to generate if-then rules for the control of bipedal robots.
However such techniques do not scale well to contact-rich systems with numerous modes.
A dynamical system with $k$ contact events can have up
to $2^k$ contact combinations. It is quite tedious, and in some cases impossible, to parameterize the effects
of these contact combinations and find individual controllers for each mode.
Moreover, it is difficult to parameterize the relationship between the states and
the conditional that triggers the control switch. 
%
In this work, we propose a control learning framework that learns a mixture of expert controllers for contact-rich systems.
%
This technique infers the experts and a gating network, which determines the control switching scheme based on observed states.
%
Moreover, we learn optimal expert controllers by injecting a performance objective that characterizes the desired behavior of the resulting closed-loop system.


The other complication in the control of hybrid systems is that they operate in an environment that is not known completely or modeled accurately.
%
For instance, a legged robot needs to perform robustly in uneven terrain. Similarly, manipulators need to hold a firm grip on objects of all textures and shapes.
%
There are techniques that combine tools from optimization, probability theory,
and machine learning to learn control strategies from inaccurate system models
or even unknown dynamics.
%
Model-free reinforcement learning is an example of a technique that relies on
repeated interactions with the unknown
environment~\cite{heess2017emergence,andrychowicz2020learning,lillicrap2015continuous}.
%
While this technique offers more flexibility on how the control policies are
inferred from unknown dynamics, they do not provide the physical structure
required to infer stability properties.
%
Thus, data-driven techniques trained in simulation, such as neural
passivity-based control (\textsc{NeuralPbc}~\cite{neuralpbc} and
\textsc{Neural-Idapbc}~\cite{neuralidapbc}) offer more insight on the stability
of the system and just as capable control strategies.
%
Nevertheless, training in simulation relies strongly on the
dynamical model, and the use of inaccurate models may lead to poor performance
or even instability.
%
This raises concerns regarding model uncertainties, especially in hybrid dynamical
systems.
%

Bayesian learning (BL)~\cite{gal2016improving,thakur} offers an alternative method to
simultaneously combat model uncertainties while preserving the useful physical
structure in a data-driven framework.
%
A common approach is shown in~\cite{sadigh2015safe, shen2022online,
pmlr-v54-linderman17a}, where a stochastic dynamical model is constructed via BL
techniques, and utilized in data-driven control synthesis executed in
simulation. 
%
For instance, BL is used to model uncertainties caused by disturbances, such as
the effect of wind gusts on quadcopters and the motion of other vehicles in
autonomous driving~\cite{sadigh2015safe}. 
%
% Nonlinear dynamics can be decomposed into segments of linear dynamics, and learn
% the linear dynamical units and their transition probabilities through
% BL~\cite{pmlr-v54-linderman17a}. 
The authors of~\cite{shen2022online} perform Bayesian learning online to
characterize time-varying kinematic and dynamical models of industrial robots.
%
Another approach to combating model uncertainties is to surpass the process of
learning the stochastic model and instead directly learn the controller through
BL. 
%
Adaptive control framework is provided in~\cite{fan2020bayesian}, where the
search for the control is given by a quadratic program that imposes
Lyapunov stability constraint for safety critical systems.
%
This technique uses BL to infer a controller through interactions with an
unknown dynamics, while maintaining the algebraic structure of a stable system.
%
Inspired by this technique, we merge the structure and stability properties of
passivity-based control (PBC) with the robustness properties of BL.  


In this work, we present a unified framework that simultaneously combine
deterministic data-driven techniques and rigorously address model uncertainties
using Bayesian learning.
%
We aim to apply BL and develop an algorithm that finds a suitable probability
distribution of the target function/policy automatically.
%
In contrast to deterministic optimization, this approach provides a probability
distribution over the parameters of the target function instead of point
estimates, providing a way to reason about model uncertainties and measurement
noise during the learning process.
%